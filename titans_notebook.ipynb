{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "definitions",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the LongTermMemory module\n",
    "class LongTermMemory(nn.Module):\n",
    "    def __init__(self, input_dim, memory_dim, learn_rate=0.1):\n",
    "        super(LongTermMemory, self).__init__()\n",
    "        self.memory = nn.Parameter(torch.zeros(memory_dim, input_dim))\n",
    "        self.learn_rate = learn_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_length, input_dim]\n",
    "        mem_exp = self.memory.unsqueeze(0).unsqueeze(0)  # [1, 1, memory_dim, input_dim]\n",
    "        x_exp = x.unsqueeze(2)  # [batch, seq_length, 1, input_dim]\n",
    "        diff = x_exp - mem_exp  # [batch, seq_length, memory_dim, input_dim]\n",
    "        surprise = torch.norm(diff, dim=-1, keepdim=True)  # [batch, seq_length, memory_dim, 1]\n",
    "        update = self.learn_rate * surprise * diff  # [batch, seq_length, memory_dim, input_dim]\n",
    "        aggregated_update = update.mean(dim=(0,1))  # [memory_dim, input_dim]\n",
    "        self.memory.data += aggregated_update\n",
    "        return self.memory\n",
    "\n",
    "# Define the Titans model\n",
    "class Titans(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, memory_dim):\n",
    "        super(Titans, self).__init__()\n",
    "        self.memory_module = LongTermMemory(input_dim, memory_dim)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=input_dim, num_heads=4, batch_first=True)\n",
    "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_length, input_dim]\n",
    "        _ = self.memory_module(x)  # update memory\n",
    "        mem = self.memory_module.memory.unsqueeze(0).expand(x.size(0), -1, -1)  # [batch, memory_dim, input_dim]\n",
    "        attn_out, _ = self.attention(query=x, key=mem, value=mem)  # [batch, seq_length, input_dim]\n",
    "        return F.relu(self.fc(attn_out))  # [batch, seq_length, hidden_dim]\n",
    "\n",
    "def generate_sequence(seq_length=50, input_dim=128, batch_size=1):\n",
    "    # Returns a tensor of shape [batch, seq_length, input_dim]\n",
    "    return torch.randn(batch_size, seq_length, input_dim)\n",
    "\n",
    "def run_model():\n",
    "    model = Titans(input_dim=128, hidden_dim=256, memory_dim=10)\n",
    "    input_seq = generate_sequence()\n",
    "    output = model(input_seq)\n",
    "    return model, output\n",
    "\n",
    "def visualize_memory(model):\n",
    "    memory_updates = model.memory_module.memory.detach().cpu().numpy()\n",
    "    plt.imshow(memory_updates, aspect='auto', cmap='viridis')\n",
    "    plt.colorbar(label='Memory Activation')\n",
    "    plt.title('Titans Memory Module Activation')\n",
    "    plt.xlabel('Feature Dimensions')\n",
    "    plt.ylabel('Memory Slots')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, output = run_model()\n",
    "visualize_memory(model)\n",
    "print('Output shape:', output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

